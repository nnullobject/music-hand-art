<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <title>Gemini Nebula - Morphing Edition</title>
    <style>
        body { margin: 0; overflow: hidden; background: #000; font-family: 'Segoe UI', sans-serif; }
        #ui { position: absolute; top: 30px; left: 30px; color: #fff; pointer-events: none; z-index: 10; transition: opacity 0.5s; }
        h1 { margin: 0; font-weight: 100; letter-spacing: 5px; font-size: 18px; color: #00f2ff; }
        .status-box { background: rgba(0,0,0,0.6); padding: 12px; border-left: 3px solid #00f2ff; margin-top: 10px; backdrop-filter: blur(10px); }
        p { margin: 4px 0; font-size: 12px; color: #00f2ff; opacity: 0.8; }
        canvas { display: block; }
    </style>
</head>
<body>
    <div id="ui">
        <h1>Gemini Visualizer / Morph</h1>
        <div class="status-box">
            <p>狀態：<span id="status">點擊螢幕啟動</span></p>
            <p>手勢狀態：<span id="hand-gesture">尚未偵測</span></p>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/three@0.150.0/build/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

    <script id="vertexShader" type="x-shader/x-vertex">
        uniform float uTime;
        uniform float uAudioHigh;
        uniform vec3 uHandPos;
        uniform float uFistStrength; // 新增：握拳強度 (0: 開掌, 1: 握拳)
        attribute float aSize;
        attribute float aIndex;
        varying vec3 vColor;

        float hash(float n) { return fract(sin(n) * 43758.5453123); }

        void main() {
            float u = (aIndex / 50000.0) * 6.28318;
            float t = uTime * 0.2;
            
            // 【核心修改 1】基礎半徑隨握拳強度收縮
            // 正常是 6.0，握拳時塌陷至 1.0
            float dynamicRadius = mix(6.0, 1.0, uFistStrength);
            float dynamicWidth = mix(2.0, 0.5, uFistStrength);
            
            float v = (sin(t + u * 2.0) + hash(aIndex) * 2.5) * dynamicWidth;

            vec3 targetPos;
            targetPos.x = cos(u) * (dynamicRadius + v * cos(u * 0.5));
            targetPos.y = sin(u) * (dynamicRadius + v * cos(u * 0.5));
            targetPos.z = v * sin(u * 0.5);

            // 【核心修改 2】引力計算與手勢點融合
            float dist = distance(targetPos, uHandPos);
            // 握拳時，引力會變得很強且具有全局性
            float attractionForce = uFistStrength * 0.8 + 0.2;
            float influence = smoothstep(8.0, 0.0, dist) * attractionForce;
            
            // 最終位置是：原始幾何位置 + 向手部收束的位移 + 音頻震動
            vec3 finalPos = mix(targetPos, uHandPos, influence * 0.6);
            finalPos += normalize(targetPos) * uAudioHigh * hash(aIndex) * 3.0;

            vec4 mvPosition = modelViewMatrix * vec4(finalPos, 1.0);
            gl_PointSize = aSize * (400.0 / -mvPosition.z) * (1.0 + uAudioHigh * 2.5);
            gl_Position = projectionMatrix * mvPosition;
            
            vColor = mix(vec3(0.1, 0.4, 1.0), vec3(1.0, 0.2, 0.6), (finalPos.z + 3.0) / 6.0);
            vColor += uAudioHigh * 0.5;
        }
    </script>

    <script id="fragmentShader" type="x-shader/x-fragment">
        varying vec3 vColor;
        void main() {
            float dist = length(gl_PointCoord - vec2(0.5));
            float strength = smoothstep(0.5, 0.2, dist);
            if (strength < 0.01) discard;
            gl_FragColor = vec4(vColor, strength * 0.8);
        }
    </script>

    <script>
        let scene, camera, renderer, particles;
        let audioAnalyser, dataArray;
        const handPos = new THREE.Vector3(0, 0, 0);
        let fistStrength = 0; // 全域握拳強度

        async function init() {
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
            camera.position.z = 18;

            renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(window.devicePixelRatio);
            renderer.setClearColor(0x000000, 1);
            document.body.appendChild(renderer.domElement);

            const count = 50000;
            const geometry = new THREE.BufferGeometry();
            const indices = new Float32Array(count);
            const sizes = new Float32Array(count);
            for(let i=0; i<count; i++) { indices[i]=i; sizes[i]=Math.random()*1.5+0.5; }
            geometry.setAttribute('aIndex', new THREE.BufferAttribute(indices, 1));
            geometry.setAttribute('aSize', new THREE.BufferAttribute(sizes, 1));
            geometry.setAttribute('position', new THREE.BufferAttribute(new Float32Array(count*3), 3));

            const material = new THREE.ShaderMaterial({
                uniforms: {
                    uTime: { value: 0 },
                    uAudioHigh: { value: 0 },
                    uHandPos: { value: handPos },
                    uFistStrength: { value: 0 }
                },
                vertexShader: document.getElementById('vertexShader').textContent,
                fragmentShader: document.getElementById('fragmentShader').textContent,
                transparent: true, blending: THREE.AdditiveBlending, depthWrite: false
            });

            particles = new THREE.Points(geometry, material);
            scene.add(particles);

            const hands = new Hands({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}` });
            hands.setOptions({ maxNumHands: 1, modelComplexity: 1, minDetectionConfidence: 0.5 });
            hands.onResults(onHandResults);

            const videoElement = document.createElement('video');
            const cameraUtils = new Camera(videoElement, {
                onFrame: async () => { await hands.send({image: videoElement}); },
                width: 640, height: 480
            });
            cameraUtils.start();

            window.addEventListener('resize', () => {
                camera.aspect = window.innerWidth / window.innerHeight; camera.updateProjectionMatrix();
                renderer.setSize(window.innerWidth, window.innerHeight);
            });
            window.onclick = initAudio;
            animate();
        }

        function onHandResults(results) {
            const statusEl = document.getElementById('hand-gesture');
            if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
                const landmarks = results.multiHandLandmarks[0];
                
                // 1. 更新手部座標 (掌心中心點 Landmark 9)
                const palm = landmarks[9];
                handPos.x = (palm.x - 0.5) * -30;
                handPos.y = (0.5 - palm.y) * 25;
                handPos.z = palm.z * -20;

                // 2. 握拳偵測邏輯
                // 計算食指、中指、無名指尖與掌心的距離
                const fingerTips = [8, 12, 16, 20];
                let avgDist = 0;
                fingerTips.forEach(idx => {
                    const dx = landmarks[idx].x - landmarks[9].x;
                    const dy = landmarks[idx].y - landmarks[9].y;
                    avgDist += Math.sqrt(dx*dx + dy*dy);
                });
                avgDist /= 4;

                // 如果平均距離小於 0.12，判定為握拳
                const targetFist = avgDist < 0.12 ? 1.0 : 0.0;
                fistStrength += (targetFist - fistStrength) * 0.1; // 平滑過渡
                
                statusEl.innerText = avgDist < 0.12 ? "✊ 握拳 (收束)" : "✋ 開掌 (舒展)";
                statusEl.style.color = avgDist < 0.12 ? "#ff0066" : "#00f2ff";
            } else {
                fistStrength += (0 - fistStrength) * 0.05;
                statusEl.innerText = "尚未偵測";
            }
        }

        async function initAudio() {
            if (audioAnalyser) return;
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                const audioCtx = new AudioContext();
                const source = audioCtx.createMediaStreamSource(stream);
                audioAnalyser = audioCtx.createAnalyser();
                audioAnalyser.fftSize = 64;
                source.connect(audioAnalyser);
                dataArray = new Uint8Array(audioAnalyser.frequencyBinCount);
                document.getElementById('status').innerText = '分析中...';
            } catch (e) { alert("請允許麥克風"); }
        }

        function animate() {
            requestAnimationFrame(animate);
            const time = performance.now() * 0.001;
            particles.material.uniforms.uTime.value = time;
            particles.material.uniforms.uFistStrength.value = fistStrength;

            if (audioAnalyser) {
                audioAnalyser.getByteFrequencyData(dataArray);
                particles.material.uniforms.uAudioHigh.value = dataArray[10] / 255.0;
            }

            particles.rotation.y = time * 0.05;
            renderer.render(scene, camera);
        }

        init();
    </script>
</body>
</html>
